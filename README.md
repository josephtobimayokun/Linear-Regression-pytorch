# Linear Regression on Synthetic Data with PyTorch

A from-scratch implementation of linear regression with **L2 regularization (weight decay)** using **PyTorch**, trained on synthetic data with VC dimension analysis and generalization bounds.

## ğŸ“Š Overview

This project demonstrates:
- **Linear regression** implemented with PyTorch's `nn.Module`
- **L2 regularization (weight decay)** to prevent overfitting
- **Synthetic data generation** with controllable parameters
- **Training/validation splits** for generalization assessment
- **Loss curve visualization** with `matplotlib`
- **VC dimension analysis** to understand generalization guarantees

## ğŸ§® Model Architecture

- **Type**: Linear regression with L2 regularization
- **Framework**: PyTorch (`nn.Linear`, `nn.MSELoss`)
- **Regularization**: L2 (weight decay in optimizer)
- **Features**: 2 input features â†’ 1 output
- **Parameters**: 3 total (2 weights + 1 bias)
- **VC Dimension**: 3 (features + 1)
- **Optimizer**: SGD with weight decay

## ğŸ“ˆ Results

### Loss Curves
![Training and Validation Loss](loss_curve.png)

*Figure 1: Training and validation loss over 50 epochs with L2 regularization (PyTorch implementation)*

### Generalization Analysis

| Metric | Value |
|--------|-------|
| Training samples | 1000 |
| VC Dimension | 3 |
| Regularization | L2 (weight decay = 0.001) |
| Confidence level | 95% (Î´ = 0.05) |
| **Generalization bound** | **~6.3%** |

**Bound calculation:**
```
Gap = âˆš((VC - log Î´)/n)
    = âˆš((3 - log 0.05)/1000)
    = âˆš(3 + 1.301/1000)
    = âˆš4.301/1000
    = 0.06324 (6.3%)
```

**Interpretation**: With 95% confidence, the difference between training error and true population error is â‰¤ 6.3%. L2 regularization helps keep this gap small by constraining model complexity.

## ğŸ—ï¸ Project Structure

```
linear_regression_on_synthetic_data/
â”œâ”€â”€ loss_curve.png                    # Training visualization
â”œâ”€â”€ data/
â”‚   â””â”€â”€ synthetic_data.py          # Data generation (NumPy â†’ PyTorch tensors)
â”œâ”€â”€ module/
â”‚   â”œâ”€â”€ module.py                   # Core
â”‚   â”œâ”€â”€ data_module.py 
â”‚   â””â”€â”€ trainer.py                   # PyTorch training loop with regularization
â”œâ”€â”€ src/
â”‚   â””â”€â”€ linear_regression_model.py   # nn.Module subclass
â”œâ”€â”€ main.py                           # Entry point
â””â”€â”€ README.md                          # You are here
```

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install torch matplotlib numpy
```

### Running the project
```bash
python main.py
```


## ğŸ“š Key Learnings

This project demonstrates understanding of:

- âœ… PyTorch `nn.Module` subclassing
- âœ… PyTorch `DataLoader` for batching
- âœ… Linear regression mathematics
- âœ… **L2 regularization via weight decay**
- âœ… Gradient descent optimization in PyTorch
- âœ… Train/validation splits
- âœ… Loss curve interpretation with matplotlib
- âœ… VC dimension theory
- âœ… Generalization bounds
- âœ… Clean code organization

## ğŸ”¬ Regularization Impact

| weight_decay | Effect |
|--------------|--------|
| 0.0 | No regularization (standard linear regression) |
| 0.0001 - 0.001 | Mild regularization |
| 0.001 - 0.01 | Balanced regularization |
| > 0.01 | Strong regularization, may underfit |

## ğŸ” Future Improvements

- [ ] Experiment with L1 regularization (via custom loss)
- [ ] Add learning rate scheduling
- [ ] Implement early stopping
- [ ] Test on real datasets
- [ ] Add tensorboard for visualization

## ğŸ“ License

MIT

## ğŸ‘¨â€ğŸ’» Author

Joseph Tobi Mayokun

---

*Built as part of my journey mastering machine learning foundationsâ€”implementing theory (VC dimension) with modern tools (PyTorch).*
